{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d062c-fbec-46b7-a24c-42663bf9a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy matplotlib gensim sentence-transformers pillow torch torchvision networkx node2vec librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f99f6-4452-49b2-8380-1f37405963fc",
   "metadata": {},
   "source": [
    " # Word Embeddings\n",
    " Word2Vec is a popular method for generating word embeddings.\n",
    " It learns vector representations of words that capture semantic relationships.\n",
    " This function turns words into numbers. It takes simple sentences like \"cat say meow\" and \"dog say woof\", and creates a list of 10 numbers for each   word. These numbers represent the meaning of the word in a way that the computer can understand. For example, the numbers for \"cat\" and \"dog\" might be similar because they're both animals.\n",
    "\n",
    " \n",
    "# Parameters:\n",
    "- vector_size=10: Dimensionality of the word vectors\n",
    "- window=5: Maximum distance between current and predicted word within a sentence\n",
    "- min_count=1: Ignores all words with total frequency lower than this\n",
    "- workers=4: Number of CPU cores to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8391ce1e-4bff-4f07-b61e-e40a4a33a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f67551c-267b-4c16-b67a-8efa61b3f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding for 'cat': [-0.0960355   0.05007293 -0.08759586 -0.04391825 -0.000351   -0.00296181\n",
      " -0.0766124   0.09614743  0.04982058  0.09233143]\n",
      "Word Embedding for 'dog': [ 0.07311766  0.05070262  0.06757693  0.00762866  0.06350891 -0.03405366\n",
      " -0.00946401  0.05768573 -0.07521638 -0.03936104]\n"
     ]
    }
   ],
   "source": [
    "def word_embeddings():\n",
    "    sentences = [['cat', 'say', 'meow'], ['dog', 'say', 'woof']]\n",
    "    model = Word2Vec(sentences, vector_size=10, window=5, min_count=1, workers=4)\n",
    "    print(\"Word Embedding for 'cat':\", model.wv['cat'])\n",
    "    print(\"Word Embedding for 'dog':\", model.wv['dog'])\n",
    "\n",
    "word_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628c01f-0e07-4372-82af-ba17a26829fb",
   "metadata": {},
   "source": [
    "# Sentence Embeddings\n",
    "SentenceTransformer is a library for state-of-the-art sentence embeddings.\n",
    "It's based on BERT architecture and fine-tuned for generating sentence embeddings.\n",
    "'paraphrase-MiniLM-L6-v2' is the name of the pre-trained model being used.\n",
    "\n",
    "This function does the same thing, but for entire sentences. It takes sentences like \"This is an example sentence\" and turns them into a list of numbers. These numbers represent the meaning of the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c993c471-5d15-4b4a-a092-4dc9256b85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d88dad9-7cd8-453e-8293-d114b83aeeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embedding shape: (2, 384)\n",
      "First sentence embedding: [0.06735881 0.783936   0.27001837 0.0958027  0.38993028]\n"
     ]
    }
   ],
   "source": [
    "def sentence_embeddings():\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    sentences = [\"This is an example sentence\", \"Each sentence is converted to a vector\"]\n",
    "    embeddings = model.encode(sentences)\n",
    "    print(\"Sentence Embedding shape:\", embeddings.shape)\n",
    "    print(\"First sentence embedding:\", embeddings[0][:5])  # First 5 dimensions\n",
    "\n",
    "sentence_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c74ccd-ccc2-4b09-94c6-9c010fdf1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464e4714-a492-4b44-8a22-f6766c84219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18 is a convolutional neural network architecture\n",
    "# It's designed to handle the vanishing gradient problem in deep networks\n",
    "# pretrained=True: Uses weights pre-trained on ImageNet\n",
    "# Image transformation pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7789c9f4-cd3d-4d78-ba1e-943783130736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Embedding shape: torch.Size([512])\n",
      "First few dimensions of image embedding: tensor([0.3197, 2.5957, 1.1854, 1.3997, 1.1710])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def image_embeddings():\n",
    "    \n",
    "    model = models.resnet18(pretrained=True)\n",
    "    \n",
    "    model = torch.nn.Sequential(*(list(model.children())[:-1]))  # Remove last fully connected layer\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(\"car_img.jpg\")  \n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).squeeze()\n",
    "    \n",
    "    print(\"Image Embedding shape:\", embedding.shape)\n",
    "    print(\"First few dimensions of image embedding:\", embedding[:5])\n",
    "\n",
    "image_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec078b46-9de9-4927-a874-0da27e626655",
   "metadata": {},
   "source": [
    "# Graph Embeddings\n",
    "Node2Vec is an algorithmic framework for learning continuous feature representations for nodes in networks\n",
    "# Parameters:\n",
    "dimensions=64: Dimensionality of the node embeddings\n",
    "walk_length=30: Length of walk per source\n",
    "num_walks=200: Number of walks per source\n",
    "workers=4: Number of CPU cores to use\n",
    "Additional parameters for fit:\n",
    "- window=10: Maximum distance between the current and predicted node in the random walk\n",
    "- min_count=1: Ignores all nodes with total frequency lower than this\n",
    "- batch_words=4: Number of words to be processed in a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb825fdc-7c8c-4738-8b53-52d829fa0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f64b6d0-4ad1-42e4-8fdf-a086972c7b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Embedding for node 0: [ 2.80140370e-01 -4.83708419e-02  1.10683531e-01  5.47424797e-03\n",
      " -3.56500149e-02  5.77721186e-02 -2.94917077e-02  2.22904339e-01\n",
      "  1.14278786e-01 -2.09032863e-01  1.72905222e-01 -1.77412242e-01\n",
      "  1.81383133e-01  1.32051751e-01 -6.11430146e-02  3.27189639e-03\n",
      " -1.27715796e-01 -4.02447172e-02 -6.64173737e-02  3.43594939e-01\n",
      "  8.59349594e-02  6.46189377e-02  2.36072257e-01 -2.51780897e-02\n",
      " -1.30023420e-01  1.97087064e-01 -9.22510307e-03 -1.71496764e-01\n",
      "  1.42524719e-01 -4.09370810e-02  2.30351612e-02 -5.56365959e-02\n",
      " -1.49412230e-01 -2.85502791e-01 -2.87243966e-02  1.11532755e-01\n",
      " -2.01834161e-02  8.94406140e-02  2.18878105e-01  2.12365031e-01\n",
      "  1.30990177e-01 -1.60938129e-01  1.24140881e-01 -1.17639706e-01\n",
      " -6.62390068e-02 -7.92074949e-02 -1.15524307e-02 -3.65299702e-01\n",
      " -6.13898635e-02  5.93779162e-02  9.47233886e-02  1.95790544e-01\n",
      " -2.49083241e-04  2.34282553e-01  6.25202134e-02  1.16826773e-01\n",
      "  1.99904129e-01 -1.85254037e-01  3.18258643e-01 -3.49126816e-01\n",
      "  1.13915071e-01 -1.32092573e-02 -1.16553918e-01  4.53611314e-02]\n"
     ]
    }
   ],
   "source": [
    "def graph_embeddings():\n",
    "    G = nx.karate_club_graph()\n",
    "    \n",
    "    node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4, quiet=True)  \n",
    "    # quiet=True disables tqdm.notebook\n",
    "    \n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    print(\"Graph Embedding for node 0:\", model.wv['0'])\n",
    "    \n",
    "graph_embeddings()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64251e2a-b9b3-4cb1-8dfb-a0ef8a0349a1",
   "metadata": {},
   "source": [
    "# Audio Embeddings\n",
    "This uses Mel-frequency cepstral coefficients (MFCCs)\n",
    "MFCCs are commonly used features in speech and audio processing\n",
    "# Parameters:\n",
    " - y: The input audio time series\n",
    " - sr: The sampling rate of y\n",
    " - n_mfcc=13: The number of MFCCs to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "032439a5-3952-481b-898e-d1b754c21e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46ed0478-c81b-4ab7-b9be-259a3adb1f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Embedding (MFCC): [-4.6306921e+02  1.4450919e+02  4.0705360e+01  4.2874289e+00\n",
      " -7.7825103e+00 -1.4643743e+01 -1.3492144e+01 -8.2774878e+00\n",
      " -1.9758449e+00  1.6375064e+00  2.8817804e+00  2.0075905e+00\n",
      "  1.2477791e-01]\n"
     ]
    }
   ],
   "source": [
    "def audio_embeddings():\n",
    "    \n",
    "    audio_path = \"sample_auadio.wav\"\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    mfcc_embedding = np.mean(mfccs.T, axis=0)\n",
    "    \n",
    "    print(\"Audio Embedding (MFCC):\", mfcc_embedding)\n",
    "\n",
    "audio_embeddings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
